{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f6fd121bb4cbc74",
   "metadata": {},
   "source": [
    "# Qwen3ã§ã®phiæ¸¬å®š\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Qwen3ãƒ¢ãƒ‡ãƒ«ã®macOSç’°å¢ƒã§ã®å‹•ä½œã‚’æ¤œè¨¼ã—ã€ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹Phiã®å€¤ã®æŒ™å‹•ã‚’ç¢ºèªã—ã¾ã™ã€‚\n",
    "\n",
    "## æ¤œè¨¼é …ç›®\n",
    "1. ğŸ¤– Qwen3ã®å‹•ä½œç¢ºèªï¼ˆchat_templateã®é©ç”¨ãªã—ï¼‰\n",
    "2. ğŸ”„ chat_templateã®é©ç”¨ï¼ˆchat_templateã®é©ç”¨æ–¹æ³•ã‚’ç¢ºèªã€‚çµæœãŒã©ã®ã‚ˆã†ã«ã‹ã‚ã‚‹ã‹ã‚’è¦³å¯Ÿï¼‰\n",
    "3. ğŸ§  Thinking Modeã®é©ç”¨ï¼ˆthinkin modelã®æœ‰ã‚Šç„¡ã—ã§ã®é•ã„ã®ç¢ºèªï¼‰\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5bd1bc4ca05e5c",
   "metadata": {},
   "source": [
    "## ğŸ›  Step 0: ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a88c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s20406/script/seminar/icml/PhiMesaSI/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f6cad2b5ed2fec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T05:48:57.823082Z",
     "start_time": "2025-07-22T05:48:54.410129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ãƒ¢ãƒ‡ãƒ«`Qwen/Qwen3-0.6B`èª­ã¿è¾¼ã¿å®Œäº† in 2.00ç§’\n"
     ]
    }
   ],
   "source": [
    "# Step 0: ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n",
    "export_dir = \"result\"\n",
    "t0 = time.time()\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"âœ… ãƒ¢ãƒ‡ãƒ«`{model_name}`èª­ã¿è¾¼ã¿å®Œäº† in {time.time() - t0:.2f}ç§’\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab01ab33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Mutual information calculation functions loaded!\n",
      "ğŸ“ Available methods:\n",
      "   - 'knn': k-Nearest Neighbors method (recommended for continuous variables)\n",
      "   - 'discretized': Binning method (traditional approach)\n",
      "ğŸ’¡ The 'knn' method is more appropriate for continuous hidden states!\n"
     ]
    }
   ],
   "source": [
    "# Mutual Information Calculation Functions\n",
    "def calculate_mutual_information_knn(state1, state2, k=3):\n",
    "    \"\"\"\n",
    "    Calculate mutual information between two hidden states using k-NN method.\n",
    "    This is more appropriate for continuous variables than discretization.\n",
    "    \n",
    "    Args:\n",
    "        state1, state2: torch tensors representing hidden states\n",
    "        k: number of nearest neighbors for estimation\n",
    "    \n",
    "    Returns:\n",
    "        float: mutual information value\n",
    "    \"\"\"\n",
    "    # Flatten the states to 1D\n",
    "    flat1 = state1.flatten().detach().cpu().numpy()\n",
    "    flat2 = state2.flatten().detach().cpu().numpy()\n",
    "    \n",
    "    # Ensure same length\n",
    "    min_len = min(len(flat1), len(flat2))\n",
    "    flat1 = flat1[:min_len]\n",
    "    flat2 = flat2[:min_len]\n",
    "    \n",
    "    # Use sklearn's mutual_info_regression for continuous variables\n",
    "    # Reshape for sklearn (samples, features)\n",
    "    X = flat1.reshape(-1, 1)\n",
    "    y = flat2\n",
    "    \n",
    "    # Calculate mutual information\n",
    "    mi = mutual_info_regression(X, y, discrete_features=False, random_state=42)[0]\n",
    "    \n",
    "    return mi\n",
    "\n",
    "def calculate_mutual_information_discretized(state1, state2, bins=50):\n",
    "    \"\"\"\n",
    "    Calculate mutual information using discretization (binning) method.\n",
    "    This is the original method, kept for comparison.\n",
    "    \n",
    "    Args:\n",
    "        state1, state2: torch tensors representing hidden states\n",
    "        bins: number of bins for discretization\n",
    "    \n",
    "    Returns:\n",
    "        float: mutual information value\n",
    "    \"\"\"\n",
    "    # Flatten the states to 1D\n",
    "    flat1 = state1.flatten().detach().cpu().numpy()\n",
    "    flat2 = state2.flatten().detach().cpu().numpy()\n",
    "    \n",
    "    # Ensure same length\n",
    "    min_len = min(len(flat1), len(flat2))\n",
    "    flat1 = flat1[:min_len]\n",
    "    flat2 = flat2[:min_len]\n",
    "    \n",
    "    # Discretize the continuous values into bins\n",
    "    hist1, bin_edges1 = np.histogram(flat1, bins=bins)\n",
    "    hist2, bin_edges2 = np.histogram(flat2, bins=bins)\n",
    "    \n",
    "    # Create joint histogram\n",
    "    joint_hist, _, _ = np.histogram2d(flat1, flat2, bins=bins)\n",
    "    \n",
    "    # Normalize to get probabilities\n",
    "    joint_prob = joint_hist / np.sum(joint_hist)\n",
    "    prob1 = hist1 / np.sum(hist1)\n",
    "    prob2 = hist2 / np.sum(hist2)\n",
    "    \n",
    "    # Calculate mutual information\n",
    "    mi = 0.0\n",
    "    for i in range(bins):\n",
    "        for j in range(bins):\n",
    "            if joint_prob[i, j] > 0 and prob1[i] > 0 and prob2[j] > 0:\n",
    "                mi += joint_prob[i, j] * np.log2(joint_prob[i, j] / (prob1[i] * prob2[j]))\n",
    "    \n",
    "    return mi\n",
    "\n",
    "def compute_mutual_information_matrix(hidden_states_list, method='knn', split_index=1):\n",
    "    \"\"\"\n",
    "    Compute nÃ—n mutual information matrix for n hidden states.\n",
    "    \n",
    "    Args:\n",
    "        hidden_states_list: list of hidden states (tensors)\n",
    "        method: 'knn' for k-nearest neighbors, 'discretized' for binning, or 'phi' for integrated information\n",
    "        split_index: integer index to split states for phi calculation (default 1)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: nÃ—n mutual information matrix\n",
    "    \"\"\"\n",
    "    n = len(hidden_states_list)\n",
    "    mi_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                # Self-mutual information (entropy)\n",
    "                flat_state = hidden_states_list[i].flatten().detach().cpu().numpy()\n",
    "                hist, _ = np.histogram(flat_state, bins=50)\n",
    "                prob = hist / np.sum(hist)\n",
    "                prob = prob[prob > 0]  # Remove zero probabilities\n",
    "                mi_matrix[i, j] = entropy(prob, base=2)\n",
    "            else:\n",
    "                if method == 'knn':\n",
    "                    mi_matrix[i, j] = calculate_mutual_information_knn(\n",
    "                        hidden_states_list[i], \n",
    "                        hidden_states_list[j]\n",
    "                    )\n",
    "                else:  # discretized\n",
    "                    mi_matrix[i, j] = calculate_mutual_information_discretized(\n",
    "                        hidden_states_list[i], \n",
    "                        hidden_states_list[j]\n",
    "                    )\n",
    "    \n",
    "    return mi_matrix\n",
    "\n",
    "def visualize_mi_matrix(mi_matrix, title=\"Mutual Information Matrix\", save_path=None):\n",
    "    \"\"\"Visualize the mutual information matrix as a heatmap.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    im = plt.imshow(mi_matrix, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(im, label='Mutual Information')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('State Index')\n",
    "    plt.ylabel('State Index')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(mi_matrix.shape[0]):\n",
    "        for j in range(mi_matrix.shape[1]):\n",
    "            plt.text(j, i, f'{mi_matrix[i, j]:.3f}', \n",
    "                    ha='center', va='center', color='white', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if path is provided\n",
    "    if save_path:\n",
    "        os.makedirs(export_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(export_dir, save_path), dpi=300, bbox_inches='tight')\n",
    "        print(f\"ğŸ“Š Figure saved to: {os.path.join(export_dir, save_path)}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ… Mutual information calculation functions loaded!\")\n",
    "print(\"ğŸ“ Available methods:\")\n",
    "print(\"   - 'knn': k-Nearest Neighbors method (recommended for continuous variables)\")\n",
    "print(\"   - 'discretized': Binning method (traditional approach)\")\n",
    "print(\"ğŸ’¡ The 'knn' method is more appropriate for continuous hidden states!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8a25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_prompt = \"Tell me a truth about artificial intelligence. Can you explain what it is?\"\n",
    "lie_prompt = \"Tell me a lie about artificial intelligence. Can you explain what it is?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3007e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RESPONSES_PER_OBJECT = 10\n",
    "NUM_OBJECTS_TRAIN = 5\n",
    "NUM_OBJECTS_VALID = 2\n",
    "\n",
    "objects = [\n",
    "    \"human brain\",\n",
    "    \"quantum computing\",\n",
    "    \"climate change\",\n",
    "    \"space exploration\",\n",
    "    \"chromosomes\",\n",
    "    \"black holes\",\n",
    "    \"genetic engineering\",\n",
    "    \"nanotechnology\",\n",
    "    \"renewable energy\",\n",
    "    \"artificial intelligence\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d4zgz6pood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—„ï¸ Initializing data collection structures...\n",
      "âœ… Data structures initialized for 10 objects\n",
      "   Each object will have 10 truth responses and 10 lie responses\n"
     ]
    }
   ],
   "source": [
    "# Data structures for storing multiple responses\n",
    "print(\"ğŸ—„ï¸ Initializing data collection structures...\")\n",
    "\n",
    "# Dictionary to store all responses: object -> prompt_type -> response_idx -> response_text\n",
    "all_responses = {}\n",
    "\n",
    "# Dictionary to store hidden states: object -> prompt_type -> response_idx -> hidden_states\n",
    "all_hidden_states = {}\n",
    "\n",
    "# Dictionary to store MI matrices: object -> prompt_type -> response_idx -> mi_matrix\n",
    "all_mi_matrices = {}\n",
    "\n",
    "# Initialize for each object\n",
    "for obj in objects:\n",
    "    all_responses[obj] = {\"truth\": {}, \"lie\": {}}\n",
    "    all_hidden_states[obj] = {\"truth\": {}, \"lie\": {}}\n",
    "    all_mi_matrices[obj] = {\"truth\": {}, \"lie\": {}}\n",
    "\n",
    "print(f\"âœ… Data structures initialized for {len(objects)} objects\")\n",
    "print(f\"   Each object will have {NUM_RESPONSES_PER_OBJECT} truth responses and {NUM_RESPONSES_PER_OBJECT} lie responses\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ngg5zx2n5i",
   "metadata": {},
   "outputs": [],
   "source": "# Data Collection: Generate responses for all objects\nprint(\"ğŸš€ Starting data collection for all objects...\")\nprint(\"=\" * 70)\n\n# Sampling parameters for diverse responses\nTEMPERATURE = 0.8  # Controls randomness (0.7-1.0 is typical)\nTOP_P = 0.9        # Nucleus sampling threshold\n\nBATCH_SIZE = 5  # Number of prompts to process simultaneously\nPROMPT_MODE = \"01_without_template\"  # Options: \"01_without_template\", \"02_with_template\", \"03_thinking\"\n\n# Set max_new_tokens based on PROMPT_MODE\nif PROMPT_MODE == \"03_thinking\":\n    MAX_NEW_TOKENS = 1024  # Thinking mode needs more tokens\nelse:\n    MAX_NEW_TOKENS = 128  # Standard modes\n\nprint(f\"ğŸ“ Generation settings:\")\nprint(f\"   - BATCH_SIZE: {BATCH_SIZE}\")\nprint(f\"   - PROMPT_MODE: {PROMPT_MODE}\")\nprint(f\"   - MAX_NEW_TOKENS: {MAX_NEW_TOKENS}\")\nprint(f\"   - Temperature: {TEMPERATURE}, Top-p: {TOP_P}, do_sample: True\")\nprint()\n\ndef format_prompt(text, mode):\n    \"\"\"\n    Format a prompt based on the specified PROMPT_MODE.\n    \n    Args:\n        text: The raw prompt text\n        mode: One of \"01_without_template\", \"02_with_template\", \"03_thinking\"\n    \n    Returns:\n        Formatted prompt string\n    \"\"\"\n    if mode == \"01_without_template\":\n        # Raw prompt (no template)\n        return text\n    elif mode == \"02_with_template\":\n        # Chat template without thinking\n        messages = [{\"role\": \"user\", \"content\": text}]\n        return tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n            enable_thinking=False\n        )\n    elif mode == \"03_thinking\":\n        # Chat template with thinking\n        messages = [{\"role\": \"user\", \"content\": text}]\n        return tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n            enable_thinking=True\n        )\n    else:\n        raise ValueError(f\"Unknown PROMPT_MODE: {mode}\")\n\ntotal_start_time = time.time()\ntotal_responses = len(objects) * NUM_RESPONSES_PER_OBJECT * 2  # truth + lie\nresponse_count = 0\n\nfor obj_idx, obj in enumerate(objects):\n    print(f\"\\nğŸ“¦ Processing object {obj_idx + 1}/{len(objects)}: '{obj}'\")\n    print(\"-\" * 70)\n    \n    # Generate truth responses in batches\n    for batch_start in range(0, NUM_RESPONSES_PER_OBJECT, BATCH_SIZE):\n        batch_end = min(batch_start + BATCH_SIZE, NUM_RESPONSES_PER_OBJECT)\n        current_batch_size = batch_end - batch_start\n        \n        # Create batch of prompts\n        raw_prompts = [f\"Tell me a truth about {obj}. Can you explain what it is?\" \n                      for _ in range(current_batch_size)]\n        formatted_prompts = [format_prompt(p, PROMPT_MODE) for p in raw_prompts]\n        \n        # Tokenize with padding for batch processing\n        inputs = tokenizer(formatted_prompts, return_tensors=\"pt\", padding=True)\n        \n        # Generate for batch\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=MAX_NEW_TOKENS,\n                do_sample=True,\n                temperature=TEMPERATURE,\n                top_p=TOP_P,\n                output_hidden_states=True,\n                return_dict_in_generate=True\n            )\n        \n        # Store each response in batch\n        for i in range(current_batch_size):\n            response_idx = batch_start + i\n            response_count += 1\n            \n            # Decode response\n            response_text = tokenizer.decode(outputs.sequences[i], skip_special_tokens=False)\n            all_responses[obj][\"truth\"][response_idx] = response_text\n            \n            # Extract hidden states for this sequence in the batch\n            # outputs.hidden_states is a tuple of generation steps\n            # Each step contains tensors of shape [batch_size, seq_len, hidden_dim]\n            # We need to extract the i-th sequence from each step\n            sequence_hidden_states = []\n            for step in outputs.hidden_states:\n                # step is a tuple of layer outputs for this generation step\n                # Extract the i-th sequence from the last layer\n                sequence_hidden_states.append(step)\n            all_hidden_states[obj][\"truth\"][response_idx] = tuple(sequence_hidden_states)\n            \n            # Compute MI matrix from last layer states\n            last_layer_states = [step[-1][i:i+1] for step in sequence_hidden_states]\n            mi_matrix = compute_mutual_information_matrix(last_layer_states, method='knn')\n            all_mi_matrices[obj][\"truth\"][response_idx] = mi_matrix\n        \n        if batch_end % 5 == 0 or batch_end == NUM_RESPONSES_PER_OBJECT:\n            print(f\"  âœ“ Truth responses: {batch_end}/{NUM_RESPONSES_PER_OBJECT} ({response_count}/{total_responses} total)\")\n    \n    # Generate lie responses in batches\n    for batch_start in range(0, NUM_RESPONSES_PER_OBJECT, BATCH_SIZE):\n        batch_end = min(batch_start + BATCH_SIZE, NUM_RESPONSES_PER_OBJECT)\n        current_batch_size = batch_end - batch_start\n        \n        # Create batch of prompts\n        raw_prompts = [f\"Tell me a lie about {obj}. Can you explain what it is?\" \n                      for _ in range(current_batch_size)]\n        formatted_prompts = [format_prompt(p, PROMPT_MODE) for p in raw_prompts]\n        \n        # Tokenize with padding for batch processing\n        inputs = tokenizer(formatted_prompts, return_tensors=\"pt\", padding=True)\n        \n        # Generate for batch\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=MAX_NEW_TOKENS,\n                do_sample=True,\n                temperature=TEMPERATURE,\n                top_p=TOP_P,\n                output_hidden_states=True,\n                return_dict_in_generate=True\n            )\n        \n        # Store each response in batch\n        for i in range(current_batch_size):\n            response_idx = batch_start + i\n            response_count += 1\n            \n            # Decode response\n            response_text = tokenizer.decode(outputs.sequences[i], skip_special_tokens=False)\n            all_responses[obj][\"lie\"][response_idx] = response_text\n            \n            # Extract hidden states for this sequence in the batch\n            sequence_hidden_states = []\n            for step in outputs.hidden_states:\n                sequence_hidden_states.append(step)\n            all_hidden_states[obj][\"lie\"][response_idx] = tuple(sequence_hidden_states)\n            \n            # Compute MI matrix from last layer states\n            last_layer_states = [step[-1][i:i+1] for step in sequence_hidden_states]\n            mi_matrix = compute_mutual_information_matrix(last_layer_states, method='knn')\n            all_mi_matrices[obj][\"lie\"][response_idx] = mi_matrix\n        \n        if batch_end % 5 == 0 or batch_end == NUM_RESPONSES_PER_OBJECT:\n            print(f\"  âœ“ Lie responses: {batch_end}/{NUM_RESPONSES_PER_OBJECT} ({response_count}/{total_responses} total)\")\n    \n    print(f\"  âœ… Completed '{obj}': {NUM_RESPONSES_PER_OBJECT} truth + {NUM_RESPONSES_PER_OBJECT} lie responses\")\n\ntotal_time = time.time() - total_start_time\nprint(\"\\n\" + \"=\" * 70)\nprint(f\"ğŸ‰ Data collection complete!\")\nprint(f\"   Total objects: {len(objects)}\")\nprint(f\"   Responses per object: {NUM_RESPONSES_PER_OBJECT * 2} (truth + lie)\")\nprint(f\"   Total responses: {total_responses}\")\nprint(f\"   Time elapsed: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\nprint(f\"   Average time per response: {total_time/total_responses:.2f} seconds\")\nprint(f\"\\nğŸ’¡ Generation settings summary:\")\nprint(f\"   - Batch size: {BATCH_SIZE} prompts per batch\")\nprint(f\"   - Prompt mode: {PROMPT_MODE}\")\nprint(f\"   - Probabilistic sampling enabled (temperature={TEMPERATURE}, top_p={TOP_P})\")"
  },
  {
   "cell_type": "markdown",
   "id": "7f00021e47efc4e",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 1: Qwen3ã®å‹•ä½œç¢ºèªï¼ˆchat_templateã®é©ç”¨ãªã—ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1a: Truth prompt analysis (using collected data)\n",
    "print(\"ğŸ” Step 1a: Truth Prompt Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Use the first truth response for \"artificial intelligence\" from collected data\n",
    "obj = \"artificial intelligence\"\n",
    "response_idx = 0\n",
    "\n",
    "truth_result = all_responses[obj][\"truth\"][response_idx]\n",
    "truth_hidden_states = all_hidden_states[obj][\"truth\"][response_idx]\n",
    "\n",
    "print(f\"Using collected data: object='{obj}', response_idx={response_idx}\")\n",
    "print(f\"Truth prompt hidden states extracted: {len(truth_hidden_states)} generation steps\")\n",
    "print(f\"Each step has {len(truth_hidden_states[0])} layers\")\n",
    "print(f\"Hidden state shape for first step, first layer: {truth_hidden_states[0][0].shape}\")\n",
    "\n",
    "# print the truth prompt and result\n",
    "truth_prompt_used = f\"Tell me a truth about {obj}. Can you explain what it is?\"\n",
    "print(f\"\\nğŸ“ Truth prompt:\\n{truth_prompt_used}\")\n",
    "print(\"=\" * 50)\n",
    "# Extract only the generated part (after the prompt)\n",
    "generated_part = truth_result[len(truth_prompt_used):]\n",
    "print(f\"ğŸ“¤ Truth result:\\n{generated_part}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1b: Lie prompt analysis (using collected data)\n",
    "print(\"ğŸ” Step 1b: Lie Prompt Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Use the first lie response for \"artificial intelligence\" from collected data\n",
    "obj = \"artificial intelligence\"\n",
    "response_idx = 0\n",
    "\n",
    "lie_result = all_responses[obj][\"lie\"][response_idx]\n",
    "lie_hidden_states = all_hidden_states[obj][\"lie\"][response_idx]\n",
    "\n",
    "print(f\"Using collected data: object='{obj}', response_idx={response_idx}\")\n",
    "print(f\"Lie prompt hidden states extracted: {len(lie_hidden_states)} generation steps\")\n",
    "print(f\"Each step has {len(lie_hidden_states[0])} layers\")\n",
    "print(f\"Hidden state shape for first step, first layer: {lie_hidden_states[0][0].shape}\")\n",
    "\n",
    "# print the lie prompt and result\n",
    "lie_prompt_used = f\"Tell me a lie about {obj}. Can you explain what it is?\"\n",
    "print(f\"\\nğŸ“ Lie prompt:\\n{lie_prompt_used}\")\n",
    "print(\"=\" * 50)\n",
    "# Extract only the generated part (after the prompt)\n",
    "generated_part = lie_result[len(lie_prompt_used):]\n",
    "print(f\"ğŸ“¤ Lie result:\\n{generated_part}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc51a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual Information Analysis for Step 1 (Raw Truth vs Lie)\n",
    "print(\"ğŸ” Mutual Information Analysis - Step 1 Raw Truth vs Lie\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze truth prompt (raw)\n",
    "if 'truth_hidden_states' in locals() and len(truth_hidden_states) > 0:\n",
    "    print(\"ğŸ“Š Analyzing Truth Prompt (Raw)...\")\n",
    "    truth_last_layer_states = [step[-1] for step in truth_hidden_states]\n",
    "    mi_matrix_truth_raw = compute_mutual_information_matrix(truth_last_layer_states, method='knn')\n",
    "    print(f\"Truth raw MI matrix shape: {mi_matrix_truth_raw.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No truth raw hidden states found for Step 1 analysis\")\n",
    "\n",
    "# Analyze lie prompt (raw)\n",
    "if 'lie_hidden_states' in locals() and len(lie_hidden_states) > 0:\n",
    "    print(\"ğŸ“Š Analyzing Lie Prompt (Raw)...\")\n",
    "    lie_last_layer_states = [step[-1] for step in lie_hidden_states]\n",
    "    mi_matrix_lie_raw = compute_mutual_information_matrix(lie_last_layer_states, method='knn')\n",
    "    print(f\"Lie raw MI matrix shape: {mi_matrix_lie_raw.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No lie raw hidden states found for Step 1 analysis\")\n",
    "\n",
    "# Compare truth vs lie for Step 1 (raw)\n",
    "if 'mi_matrix_truth_raw' in locals() and 'mi_matrix_lie_raw' in locals():\n",
    "    print(f\"\\nğŸ”„ Step 1 Raw Truth vs Lie Comparison:\")\n",
    "    print(f\"Truth raw matrix shape: {mi_matrix_truth_raw.shape}\")\n",
    "    print(f\"Lie raw matrix shape: {mi_matrix_lie_raw.shape}\")\n",
    "    \n",
    "    # Handle different dimensions by using the smaller matrix size\n",
    "    min_size = min(mi_matrix_truth_raw.shape[0], mi_matrix_lie_raw.shape[0])\n",
    "    print(f\"Using first {min_size} dimensions for comparison\")\n",
    "    \n",
    "    # Truncate both matrices to the same size\n",
    "    truth_raw_truncated = mi_matrix_truth_raw[:min_size, :min_size]\n",
    "    lie_raw_truncated = mi_matrix_lie_raw[:min_size, :min_size]\n",
    "    \n",
    "    # Calculate difference matrix\n",
    "    diff_matrix_raw = lie_raw_truncated - truth_raw_truncated\n",
    "    \n",
    "    # Print comparison statistics\n",
    "    print(f\"\\nğŸ“ˆ Step 1 Raw Truth vs Lie Statistics:\")\n",
    "    print(f\"Mean MI difference (Lie - Truth): {np.mean(diff_matrix_raw):.4f}\")\n",
    "    print(f\"Max MI difference: {np.max(diff_matrix_raw):.4f}\")\n",
    "    print(f\"Min MI difference: {np.min(diff_matrix_raw):.4f}\")\n",
    "    print(f\"Std MI difference: {np.std(diff_matrix_raw):.4f}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Truth raw matrix - centered at 0\n",
    "    vmax_mi = max(np.abs(truth_raw_truncated.min()), np.abs(truth_raw_truncated.max()))\n",
    "    im1 = axes[0, 0].imshow(truth_raw_truncated, cmap='viridis', aspect='auto', vmin=-vmax_mi, vmax=vmax_mi)\n",
    "    axes[0, 0].set_title('Step 1 Truth Raw - MI Matrix')\n",
    "    axes[0, 0].set_xlabel('State Index')\n",
    "    axes[0, 0].set_ylabel('State Index')\n",
    "    plt.colorbar(im1, ax=axes[0, 0], label='Mutual Information')\n",
    "    \n",
    "    # Lie raw matrix - centered at 0\n",
    "    vmax_mi = max(np.abs(lie_raw_truncated.min()), np.abs(lie_raw_truncated.max()))\n",
    "    im2 = axes[0, 1].imshow(lie_raw_truncated, cmap='viridis', aspect='auto', vmin=-vmax_mi, vmax=vmax_mi)\n",
    "    axes[0, 1].set_title('Step 1 Lie Raw - MI Matrix')\n",
    "    axes[0, 1].set_xlabel('State Index')\n",
    "    axes[0, 1].set_ylabel('State Index')\n",
    "    plt.colorbar(im2, ax=axes[0, 1], label='Mutual Information')\n",
    "    \n",
    "    # Difference matrix - white at 0\n",
    "    vmax_diff = max(np.abs(diff_matrix_raw.min()), np.abs(diff_matrix_raw.max()))\n",
    "    im3 = axes[1, 0].imshow(diff_matrix_raw, cmap='RdBu_r', aspect='auto', vmin=-vmax_diff, vmax=vmax_diff)\n",
    "    axes[1, 0].set_title('Step 1 Raw Difference (Lie - Truth)')\n",
    "    axes[1, 0].set_xlabel('State Index')\n",
    "    axes[1, 0].set_ylabel('State Index')\n",
    "    plt.colorbar(im3, ax=axes[1, 0], label='MI Difference')\n",
    "    \n",
    "    # Statistical comparison\n",
    "    axes[1, 1].hist(diff_matrix_raw.flatten(), bins=50, alpha=0.7, color='blue')\n",
    "    axes[1, 1].set_title('Step 1 Raw MI Differences Distribution')\n",
    "    axes[1, 1].set_xlabel('MI Difference (Lie - Truth)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].axvline(0, color='red', linestyle='--', alpha=0.7, label='Zero difference')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(export_dir, 'step1_raw_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Figure saved to: {os.path.join(export_dir, 'step1_raw_comparison.png')}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Save for later comparison\n",
    "    step1_truth_raw_mi_matrix = mi_matrix_truth_raw.copy()\n",
    "    step1_lie_raw_mi_matrix = mi_matrix_lie_raw.copy()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Need both truth and lie raw MI matrices for Step 1 comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfc764917eb39a",
   "metadata": {},
   "source": [
    "## ğŸ”„ Step 2: chat_templateã®é©ç”¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340018e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2a: Truth prompt with chat_templateï¼ˆenable_thinking=Falseï¼‰\n",
    "print(\"ğŸ” Step 2a: Truth Prompt with Chat Template\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": truth_prompt}]\n",
    "chat_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,  # ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ \n",
    "    enable_thinking=False        # Thinking Modeã‚’ç„¡åŠ¹åŒ–\n",
    ")\n",
    "\n",
    "# encode the chat prompt\n",
    "inputs = tokenizer.encode(chat_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# generate the output with hidden states extraction\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_new_tokens=128, \n",
    "        do_sample=False,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "# decode the output\n",
    "truth_chat_result = tokenizer.decode(outputs.sequences[0], skip_special_tokens=False)\n",
    "\n",
    "# extract hidden states\n",
    "truth_chat_hidden_states = outputs.hidden_states\n",
    "print(f\"Truth chat template hidden states extracted: {len(truth_chat_hidden_states)} generation steps\")\n",
    "print(f\"Each step has {len(truth_chat_hidden_states[0])} layers\")\n",
    "print(f\"Hidden state shape for first step, first layer: {truth_chat_hidden_states[0][0].shape}\")\n",
    "\n",
    "# print the chat prompt and result\n",
    "print(f\"ğŸ“ Truth chat prompt:\\n{chat_prompt}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“¤ Truth chat result:\\n{truth_chat_result[len(chat_prompt):]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2b: Lie prompt with chat_templateï¼ˆenable_thinking=Falseï¼‰\n",
    "print(\"ğŸ” Step 2b: Lie Prompt with Chat Template\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": lie_prompt}]\n",
    "chat_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,  # ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ \n",
    "    enable_thinking=False        # Thinking Modeã‚’ç„¡åŠ¹åŒ–\n",
    ")\n",
    "\n",
    "# encode the chat prompt\n",
    "inputs = tokenizer.encode(chat_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# generate the output with hidden states extraction\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_new_tokens=128, \n",
    "        do_sample=False,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "# decode the output\n",
    "lie_chat_result = tokenizer.decode(outputs.sequences[0], skip_special_tokens=False)\n",
    "\n",
    "# extract hidden states\n",
    "lie_chat_hidden_states = outputs.hidden_states\n",
    "print(f\"Lie chat template hidden states extracted: {len(lie_chat_hidden_states)} generation steps\")\n",
    "print(f\"Each step has {len(lie_chat_hidden_states[0])} layers\")\n",
    "print(f\"Hidden state shape for first step, first layer: {lie_chat_hidden_states[0][0].shape}\")\n",
    "\n",
    "# print the chat prompt and result\n",
    "print(f\"ğŸ“ Lie chat prompt:\\n{chat_prompt}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“¤ Lie chat result:\\n{lie_chat_result[len(chat_prompt):]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual Information Analysis for Step 2 (Chat Template Truth vs Lie)\n",
    "print(\"ğŸ” Mutual Information Analysis - Step 2 Chat Template Truth vs Lie\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze truth prompt with chat template\n",
    "if 'truth_chat_hidden_states' in locals() and len(truth_chat_hidden_states) > 0:\n",
    "    print(\"ğŸ“Š Analyzing Truth Prompt with Chat Template...\")\n",
    "    truth_chat_last_layer_states = [step[-1] for step in truth_chat_hidden_states]\n",
    "    mi_matrix_truth_chat = compute_mutual_information_matrix(truth_chat_last_layer_states, method='knn')\n",
    "    print(f\"Truth chat MI matrix shape: {mi_matrix_truth_chat.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No truth chat hidden states found for Step 2 analysis\")\n",
    "\n",
    "# Analyze lie prompt with chat template\n",
    "if 'lie_chat_hidden_states' in locals() and len(lie_chat_hidden_states) > 0:\n",
    "    print(\"ğŸ“Š Analyzing Lie Prompt with Chat Template...\")\n",
    "    lie_chat_last_layer_states = [step[-1] for step in lie_chat_hidden_states]\n",
    "    mi_matrix_lie_chat = compute_mutual_information_matrix(lie_chat_last_layer_states, method='knn')\n",
    "    print(f\"Lie chat MI matrix shape: {mi_matrix_lie_chat.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No lie chat hidden states found for Step 2 analysis\")\n",
    "\n",
    "# Compare truth vs lie for Step 2 (chat template)\n",
    "if 'mi_matrix_truth_chat' in locals() and 'mi_matrix_lie_chat' in locals():\n",
    "    print(f\"\\nğŸ”„ Step 2 Chat Template Truth vs Lie Comparison:\")\n",
    "    print(f\"Truth chat matrix shape: {mi_matrix_truth_chat.shape}\")\n",
    "    print(f\"Lie chat matrix shape: {mi_matrix_lie_chat.shape}\")\n",
    "    \n",
    "    # Handle different dimensions by using the smaller matrix size\n",
    "    min_size = min(mi_matrix_truth_chat.shape[0], mi_matrix_lie_chat.shape[0])\n",
    "    print(f\"Using first {min_size} dimensions for comparison\")\n",
    "    \n",
    "    # Truncate both matrices to the same size\n",
    "    truth_chat_truncated = mi_matrix_truth_chat[:min_size, :min_size]\n",
    "    lie_chat_truncated = mi_matrix_lie_chat[:min_size, :min_size]\n",
    "    \n",
    "    # Calculate difference matrix\n",
    "    diff_matrix_chat = lie_chat_truncated - truth_chat_truncated\n",
    "    \n",
    "    # Print comparison statistics\n",
    "    print(f\"\\nğŸ“ˆ Step 2 Chat Template Truth vs Lie Statistics:\")\n",
    "    print(f\"Mean MI difference (Lie - Truth): {np.mean(diff_matrix_chat):.4f}\")\n",
    "    print(f\"Max MI difference: {np.max(diff_matrix_chat):.4f}\")\n",
    "    print(f\"Min MI difference: {np.min(diff_matrix_chat):.4f}\")\n",
    "    print(f\"Std MI difference: {np.std(diff_matrix_chat):.4f}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Truth chat matrix - centered at 0\n",
    "    vmax_mi = max(np.abs(truth_chat_truncated.min()), np.abs(truth_chat_truncated.max()))\n",
    "    im1 = axes[0, 0].imshow(truth_chat_truncated, cmap='viridis', aspect='auto', vmin=-vmax_mi, vmax=vmax_mi)\n",
    "    axes[0, 0].set_title('Step 2 Truth Chat - MI Matrix')\n",
    "    axes[0, 0].set_xlabel('State Index')\n",
    "    axes[0, 0].set_ylabel('State Index')\n",
    "    plt.colorbar(im1, ax=axes[0, 0], label='Mutual Information')\n",
    "    \n",
    "    # Lie chat matrix - centered at 0\n",
    "    vmax_mi = max(np.abs(lie_chat_truncated.min()), np.abs(lie_chat_truncated.max()))\n",
    "    im2 = axes[0, 1].imshow(lie_chat_truncated, cmap='viridis', aspect='auto', vmin=-vmax_mi, vmax=vmax_mi)\n",
    "    axes[0, 1].set_title('Step 2 Lie Chat - MI Matrix')\n",
    "    axes[0, 1].set_xlabel('State Index')\n",
    "    axes[0, 1].set_ylabel('State Index')\n",
    "    plt.colorbar(im2, ax=axes[0, 1], label='Mutual Information')\n",
    "    \n",
    "    # Difference matrix - white at 0\n",
    "    vmax_diff = max(np.abs(diff_matrix_chat.min()), np.abs(diff_matrix_chat.max()))\n",
    "    im3 = axes[1, 0].imshow(diff_matrix_chat, cmap='RdBu_r', aspect='auto', vmin=-vmax_diff, vmax=vmax_diff)\n",
    "    axes[1, 0].set_title('Step 2 Chat Difference (Lie - Truth)')\n",
    "    axes[1, 0].set_xlabel('State Index')\n",
    "    axes[1, 0].set_ylabel('State Index')\n",
    "    plt.colorbar(im3, ax=axes[1, 0], label='MI Difference')\n",
    "    \n",
    "    # Statistical comparison\n",
    "    axes[1, 1].hist(diff_matrix_chat.flatten(), bins=50, alpha=0.7, color='orange')\n",
    "    axes[1, 1].set_title('Step 2 Chat MI Differences Distribution')\n",
    "    axes[1, 1].set_xlabel('MI Difference (Lie - Truth)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].axvline(0, color='red', linestyle='--', alpha=0.7, label='Zero difference')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(export_dir, 'step2_chat_template_comparison_v2.png'), dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Figure saved to: {os.path.join(export_dir, 'step2_chat_template_comparison_v2.png')}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Save for later comparison\n",
    "    step2_truth_chat_mi_matrix = mi_matrix_truth_chat.copy()\n",
    "    step2_lie_chat_mi_matrix = mi_matrix_lie_chat.copy()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Need both truth and lie chat MI matrices for Step 2 comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e13812eba7c6a0",
   "metadata": {},
   "source": [
    "## ğŸ§  Step 3. Thinking Modeã®é©ç”¨\n",
    "Thinking Modeã‚’æœ‰åŠ¹ã«ã—ã¦ã€æ€è€ƒéç¨‹ã‚’å«ã‚ãŸå¿œç­”ã‚’ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1583510e3fe39933",
   "metadata": {},
   "source": [
    "ä¸Šè¨˜ã®è¨­å®šï¼ˆmax_new_tokens=32ï¼‰ã ã¨ã€ãŠãã‚‰ãthinkingéƒ¨åˆ†ãŒé€”ä¸­ã§åˆ‡ã‚Œã¦ã—ã¦ã—ã¾ã†ã®ã§ã€max_new_tokensã‚’å¢—ã‚„ã—ã¦å†å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a: Truth prompt with thinking modeï¼ˆenable_thinking=Trueï¼‰\n",
    "print(\"ğŸ” Step 3a: Truth Prompt with Thinking Mode\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# max_new_tokens = 512  # Increased for thinking mode\n",
    "max_new_tokens = 1024  # Increased for thinking mode\n",
    "\n",
    "# Chat templateé©ç”¨æ™‚ã«ã€thinking modeã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
    "thinking_prompt_truth = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": truth_prompt}], \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True, \n",
    "    enable_thinking=True\n",
    ")\n",
    "\n",
    "# encode the thinking prompt\n",
    "inputs = tokenizer.encode(thinking_prompt_truth, return_tensors=\"pt\")\n",
    "\n",
    "# generate the output with hidden states extraction\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        do_sample=False,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "# decode the output\n",
    "thinking_result_truth = tokenizer.decode(outputs.sequences[0], skip_special_tokens=False)\n",
    "\n",
    "# extract hidden states\n",
    "thinking_hidden_states_truth = outputs.hidden_states\n",
    "print(f\"Truth thinking mode hidden states extracted: {len(thinking_hidden_states_truth)} generation steps\")\n",
    "print(f\"Each step has {len(thinking_hidden_states_truth[0])} layers\")\n",
    "print(f\"Hidden state shape for first step, first layer: {thinking_hidden_states_truth[0][0].shape}\")\n",
    "\n",
    "# print the thinking prompt and result\n",
    "print(f\"ğŸ“ Truth thinking prompt:\\n{thinking_prompt_truth}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“¤ Truth thinking result:\\n{thinking_result_truth[len(thinking_prompt_truth):]}\")\n",
    "\n",
    "# Extract thinking and response parts\n",
    "\n",
    "match = re.search(r'<think>(.*?)</think>(.*)', thinking_result_truth, flags=re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    print(f\"\\nğŸ§  Truth Thinking:\\n{match.group(1).strip()}\\n\")\n",
    "    print(f\"ğŸ’¬ Truth Response:\\n{match.group(2).strip()}\")\n",
    "elif '<think>' in thinking_result_truth:\n",
    "    think_start = thinking_result_truth.find('<think>') + 7\n",
    "    thinking_part = thinking_result_truth[think_start:]\n",
    "    print(f\"\\nğŸ§  Truth Thinking (incomplete):\\n{thinking_part.strip()}\")\n",
    "    print(\"âš ï¸ Thinking generation was truncated\")\n",
    "else:\n",
    "    print(\"âš ï¸ No thinking format detected in truth response\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02692a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b: Lie prompt with thinking modeï¼ˆenable_thinking=Trueï¼‰\n",
    "print(\"ğŸ” Step 3b: Lie Prompt with Thinking Mode\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Chat templateé©ç”¨æ™‚ã«ã€thinking modeã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
    "thinking_prompt_lie = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": lie_prompt}], \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True, \n",
    "    enable_thinking=True\n",
    ")\n",
    "\n",
    "# encode the thinking prompt\n",
    "inputs = tokenizer.encode(thinking_prompt_lie, return_tensors=\"pt\")\n",
    "\n",
    "# generate the output with hidden states extraction\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        do_sample=False,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "# decode the output\n",
    "thinking_result_lie = tokenizer.decode(outputs.sequences[0], skip_special_tokens=False)\n",
    "\n",
    "# extract hidden states\n",
    "thinking_hidden_states_lie = outputs.hidden_states\n",
    "print(f\"Lie thinking mode hidden states extracted: {len(thinking_hidden_states_lie)} generation steps\")\n",
    "print(f\"Each step has {len(thinking_hidden_states_lie[0])} layers\")\n",
    "print(f\"Hidden state shape for first step, first layer: {thinking_hidden_states_lie[0][0].shape}\")\n",
    "\n",
    "# print the thinking prompt and result\n",
    "print(f\"ğŸ“ Lie thinking prompt:\\n{thinking_prompt_lie}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“¤ Lie thinking result:\\n{thinking_result_lie[len(thinking_prompt_lie):]}\")\n",
    "\n",
    "# Extract thinking and response parts\n",
    "match = re.search(r'<think>(.*?)</think>(.*)', thinking_result_lie, flags=re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    print(f\"\\nğŸ§  Lie Thinking:\\n{match.group(1).strip()}\\n\")\n",
    "    print(f\"ğŸ’¬ Lie Response:\\n{match.group(2).strip()}\")\n",
    "elif '<think>' in thinking_result_lie:\n",
    "    think_start = thinking_result_lie.find('<think>') + 7\n",
    "    thinking_part = thinking_result_lie[think_start:]\n",
    "    print(f\"\\nğŸ§  Lie Thinking (incomplete):\\n{thinking_part.strip()}\")\n",
    "    print(\"âš ï¸ Thinking generation was truncated\")\n",
    "else:\n",
    "    print(\"âš ï¸ No thinking format detected in lie response\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd76696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual Information Analysis for Step 3 (Thinking Mode Truth vs Lie)\n",
    "print(\"ğŸ” Mutual Information Analysis - Step 3 Thinking Mode Truth vs Lie\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze truth prompt with thinking mode\n",
    "if 'thinking_hidden_states_truth' in locals() and len(thinking_hidden_states_truth) > 0:\n",
    "    print(\"ğŸ“Š Analyzing Truth Prompt with Thinking Mode...\")\n",
    "    truth_thinking_last_layer_states = [step[-1] for step in thinking_hidden_states_truth]\n",
    "    mi_matrix_truth_thinking = compute_mutual_information_matrix(truth_thinking_last_layer_states, method='knn')\n",
    "    print(f\"Truth thinking MI matrix shape: {mi_matrix_truth_thinking.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No truth thinking hidden states found for Step 3 analysis\")\n",
    "\n",
    "# Analyze lie prompt with thinking mode\n",
    "if 'thinking_hidden_states_lie' in locals() and len(thinking_hidden_states_lie) > 0:\n",
    "    print(\"ğŸ“Š Analyzing Lie Prompt with Thinking Mode...\")\n",
    "    lie_thinking_last_layer_states = [step[-1] for step in thinking_hidden_states_lie]\n",
    "    mi_matrix_lie_thinking = compute_mutual_information_matrix(lie_thinking_last_layer_states, method='knn')\n",
    "    print(f\"Lie thinking MI matrix shape: {mi_matrix_lie_thinking.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No lie thinking hidden states found for Step 3 analysis\")\n",
    "\n",
    "# Compare truth vs lie for Step 3 (thinking mode)\n",
    "if 'mi_matrix_truth_thinking' in locals() and 'mi_matrix_lie_thinking' in locals():\n",
    "    print(f\"\\nğŸ”„ Step 3 Thinking Mode Truth vs Lie Comparison:\")\n",
    "    print(f\"Truth thinking matrix shape: {mi_matrix_truth_thinking.shape}\")\n",
    "    print(f\"Lie thinking matrix shape: {mi_matrix_lie_thinking.shape}\")\n",
    "    \n",
    "    # Handle different dimensions by using the smaller matrix size\n",
    "    min_size = min(mi_matrix_truth_thinking.shape[0], mi_matrix_lie_thinking.shape[0])\n",
    "    print(f\"Using first {min_size} dimensions for comparison\")\n",
    "    \n",
    "    # Truncate both matrices to the same size\n",
    "    truth_thinking_truncated = mi_matrix_truth_thinking[:min_size, :min_size]\n",
    "    lie_thinking_truncated = mi_matrix_lie_thinking[:min_size, :min_size]\n",
    "    \n",
    "    # Calculate difference matrix\n",
    "    diff_matrix_thinking = lie_thinking_truncated - truth_thinking_truncated\n",
    "    \n",
    "    # Print comparison statistics\n",
    "    print(f\"\\nğŸ“ˆ Step 3 Thinking Mode Truth vs Lie Statistics:\")\n",
    "    print(f\"Mean MI difference (Lie - Truth): {np.mean(diff_matrix_thinking):.4f}\")\n",
    "    print(f\"Max MI difference: {np.max(diff_matrix_thinking):.4f}\")\n",
    "    print(f\"Min MI difference: {np.min(diff_matrix_thinking):.4f}\")\n",
    "    print(f\"Std MI difference: {np.std(diff_matrix_thinking):.4f}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Truth thinking matrix - centered at 0\n",
    "    vmax_mi = max(np.abs(truth_thinking_truncated.min()), np.abs(truth_thinking_truncated.max()))\n",
    "    im1 = axes[0, 0].imshow(truth_thinking_truncated, cmap='viridis', aspect='auto', vmin=-vmax_mi, vmax=vmax_mi)\n",
    "    axes[0, 0].set_title('Step 3 Truth Thinking - MI Matrix')\n",
    "    axes[0, 0].set_xlabel('State Index')\n",
    "    axes[0, 0].set_ylabel('State Index')\n",
    "    plt.colorbar(im1, ax=axes[0, 0], label='Mutual Information')\n",
    "    \n",
    "    # Lie thinking matrix - centered at 0\n",
    "    vmax_mi = max(np.abs(lie_thinking_truncated.min()), np.abs(lie_thinking_truncated.max()))\n",
    "    im2 = axes[0, 1].imshow(lie_thinking_truncated, cmap='viridis', aspect='auto', vmin=-vmax_mi, vmax=vmax_mi)\n",
    "    axes[0, 1].set_title('Step 3 Lie Thinking - MI Matrix')\n",
    "    axes[0, 1].set_xlabel('State Index')\n",
    "    axes[0, 1].set_ylabel('State Index')\n",
    "    plt.colorbar(im2, ax=axes[0, 1], label='Mutual Information')\n",
    "    \n",
    "    # Difference matrix - white at 0\n",
    "    vmax_diff = max(np.abs(diff_matrix_thinking.min()), np.abs(diff_matrix_thinking.max()))\n",
    "    im3 = axes[1, 0].imshow(diff_matrix_thinking, cmap='RdBu_r', aspect='auto', vmin=-vmax_diff, vmax=vmax_diff)\n",
    "    axes[1, 0].set_title('Step 3 Thinking Difference (Lie - Truth)')\n",
    "    axes[1, 0].set_xlabel('State Index')\n",
    "    axes[1, 0].set_ylabel('State Index')\n",
    "    plt.colorbar(im3, ax=axes[1, 0], label='MI Difference')\n",
    "    \n",
    "    # Statistical comparison\n",
    "    axes[1, 1].hist(diff_matrix_thinking.flatten(), bins=50, alpha=0.7, color='green')\n",
    "    axes[1, 1].set_title('Step 3 Thinking MI Differences Distribution')\n",
    "    axes[1, 1].set_xlabel('MI Difference (Lie - Truth)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].axvline(0, color='red', linestyle='--', alpha=0.7, label='Zero difference')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(export_dir, 'step3_thinking_mode_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Figure saved to: {os.path.join(export_dir, 'step3_thinking_mode_comparison.png')}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze specific patterns\n",
    "    print(f\"\\nğŸ” Step 3 Thinking Pattern Analysis:\")\n",
    "    \n",
    "    # Off-diagonal elements comparison\n",
    "    truth_thinking_off_diag = truth_thinking_truncated[np.triu_indices_from(truth_thinking_truncated, k=1)]\n",
    "    lie_thinking_off_diag = lie_thinking_truncated[np.triu_indices_from(lie_thinking_truncated, k=1)]\n",
    "    \n",
    "    print(f\"Truth thinking mean off-diagonal MI: {np.mean(truth_thinking_off_diag):.4f}\")\n",
    "    print(f\"Lie thinking mean off-diagonal MI: {np.mean(lie_thinking_off_diag):.4f}\")\n",
    "    print(f\"Difference in off-diagonal MI: {np.mean(lie_thinking_off_diag) - np.mean(truth_thinking_off_diag):.4f}\")\n",
    "    \n",
    "    # Diagonal elements comparison (entropy)\n",
    "    truth_thinking_diag = np.diag(truth_thinking_truncated)\n",
    "    lie_thinking_diag = np.diag(lie_thinking_truncated)\n",
    "    \n",
    "    print(f\"Truth thinking mean entropy (diagonal): {np.mean(truth_thinking_diag):.4f}\")\n",
    "    print(f\"Lie thinking mean entropy (diagonal): {np.mean(lie_thinking_diag):.4f}\")\n",
    "    print(f\"Difference in entropy: {np.mean(lie_thinking_diag) - np.mean(truth_thinking_diag):.4f}\")\n",
    "    \n",
    "    # Save for later comparison\n",
    "    step3_truth_thinking_mi_matrix = mi_matrix_truth_thinking.copy()\n",
    "    step3_lie_thinking_mi_matrix = mi_matrix_lie_thinking.copy()\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ Step 3 Thinking Mode Insights:\")\n",
    "    print(f\"   - Thinking mode shows internal reasoning processes\")\n",
    "    print(f\"   - Positive differences indicate higher MI in lie thinking\")\n",
    "    print(f\"   - Negative differences indicate higher MI in truth thinking\")\n",
    "    print(f\"   - Diagonal differences show entropy changes in thinking processes\")\n",
    "    print(f\"   - Off-diagonal differences show information flow in reasoning\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Need both truth and lie thinking MI matrices for Step 3 comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1m897eogrp",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 4: Classifier Training\n",
    "Train a binary classifier to distinguish between truth and lie responses based on MI matrix features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j26imrwbbm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4a: Feature extraction from MI matrices\n",
    "print(\"ğŸ”§ Step 4a: Extracting features from MI matrices\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def extract_features_from_mi_matrix(mi_matrix):\n",
    "    \"\"\"\n",
    "    Extract meaningful features from an MI matrix for classification.\n",
    "    \n",
    "    Features extracted:\n",
    "    - Mean of diagonal elements (entropy)\n",
    "    - Std of diagonal elements\n",
    "    - Mean of off-diagonal elements (mutual information)\n",
    "    - Std of off-diagonal elements\n",
    "    - Max value in matrix\n",
    "    - Min value in matrix\n",
    "    - Mean of upper triangle (excluding diagonal)\n",
    "    - Mean of lower triangle (excluding diagonal)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of features\n",
    "    \"\"\"\n",
    "    # Diagonal elements (entropy)\n",
    "    diag = np.diag(mi_matrix)\n",
    "    diag_mean = np.mean(diag)\n",
    "    diag_std = np.std(diag)\n",
    "    \n",
    "    # Off-diagonal elements\n",
    "    mask = ~np.eye(mi_matrix.shape[0], dtype=bool)\n",
    "    off_diag = mi_matrix[mask]\n",
    "    off_diag_mean = np.mean(off_diag)\n",
    "    off_diag_std = np.std(off_diag)\n",
    "    \n",
    "    # Overall statistics\n",
    "    max_val = np.max(mi_matrix)\n",
    "    min_val = np.min(mi_matrix)\n",
    "    \n",
    "    # Upper and lower triangles\n",
    "    upper_triangle = mi_matrix[np.triu_indices_from(mi_matrix, k=1)]\n",
    "    lower_triangle = mi_matrix[np.tril_indices_from(mi_matrix, k=-1)]\n",
    "    upper_mean = np.mean(upper_triangle) if len(upper_triangle) > 0 else 0\n",
    "    lower_mean = np.mean(lower_triangle) if len(lower_triangle) > 0 else 0\n",
    "    \n",
    "    features = np.array([\n",
    "        diag_mean,\n",
    "        diag_std,\n",
    "        off_diag_mean,\n",
    "        off_diag_std,\n",
    "        max_val,\n",
    "        min_val,\n",
    "        upper_mean,\n",
    "        lower_mean\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for all collected data\n",
    "all_features = []\n",
    "all_labels = []  # 0 for truth, 1 for lie\n",
    "all_object_indices = []\n",
    "\n",
    "for obj_idx, obj in enumerate(objects):\n",
    "    # Truth features\n",
    "    for response_idx in range(NUM_RESPONSES_PER_OBJECT):\n",
    "        mi_matrix = all_mi_matrices[obj][\"truth\"][response_idx]\n",
    "        features = extract_features_from_mi_matrix(mi_matrix)\n",
    "        all_features.append(features)\n",
    "        all_labels.append(0)  # truth = 0\n",
    "        all_object_indices.append(obj_idx)\n",
    "    \n",
    "    # Lie features\n",
    "    for response_idx in range(NUM_RESPONSES_PER_OBJECT):\n",
    "        mi_matrix = all_mi_matrices[obj][\"lie\"][response_idx]\n",
    "        features = extract_features_from_mi_matrix(mi_matrix)\n",
    "        all_features.append(features)\n",
    "        all_labels.append(1)  # lie = 1\n",
    "        all_object_indices.append(obj_idx)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(all_features)\n",
    "y = np.array(all_labels)\n",
    "object_indices = np.array(all_object_indices)\n",
    "\n",
    "print(f\"âœ… Feature extraction complete!\")\n",
    "print(f\"   Feature matrix shape: {X.shape}\")\n",
    "print(f\"   Labels shape: {y.shape}\")\n",
    "print(f\"   Feature names: ['diag_mean', 'diag_std', 'off_diag_mean', 'off_diag_std', 'max', 'min', 'upper_mean', 'lower_mean']\")\n",
    "print(f\"   Total samples: {len(X)}\")\n",
    "print(f\"   Truth samples: {np.sum(y == 0)}\")\n",
    "print(f\"   Lie samples: {np.sum(y == 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elqse6615j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b: Data split (train/valid/test)\n",
    "print(\"ğŸ“Š Step 4b: Splitting data into train/valid/test sets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Split based on object indices\n",
    "# Train: first NUM_OBJECTS_TRAIN objects (0-4)\n",
    "# Valid: next NUM_OBJECTS_VALID objects (5-6)\n",
    "# Test: remaining objects (7-9)\n",
    "\n",
    "train_mask = object_indices < NUM_OBJECTS_TRAIN\n",
    "valid_mask = (object_indices >= NUM_OBJECTS_TRAIN) & (object_indices < NUM_OBJECTS_TRAIN + NUM_OBJECTS_VALID)\n",
    "test_mask = object_indices >= NUM_OBJECTS_TRAIN + NUM_OBJECTS_VALID\n",
    "\n",
    "X_train = X[train_mask]\n",
    "y_train = y[train_mask]\n",
    "\n",
    "X_valid = X[valid_mask]\n",
    "y_valid = y[valid_mask]\n",
    "\n",
    "X_test = X[test_mask]\n",
    "y_test = y[test_mask]\n",
    "\n",
    "print(f\"âœ… Data split complete!\")\n",
    "print(f\"\\nğŸ“¦ Training set:\")\n",
    "print(f\"   Objects: {objects[:NUM_OBJECTS_TRAIN]}\")\n",
    "print(f\"   Shape: {X_train.shape}\")\n",
    "print(f\"   Truth samples: {np.sum(y_train == 0)}\")\n",
    "print(f\"   Lie samples: {np.sum(y_train == 1)}\")\n",
    "\n",
    "print(f\"\\nğŸ“¦ Validation set:\")\n",
    "print(f\"   Objects: {objects[NUM_OBJECTS_TRAIN:NUM_OBJECTS_TRAIN + NUM_OBJECTS_VALID]}\")\n",
    "print(f\"   Shape: {X_valid.shape}\")\n",
    "print(f\"   Truth samples: {np.sum(y_valid == 0)}\")\n",
    "print(f\"   Lie samples: {np.sum(y_valid == 1)}\")\n",
    "\n",
    "print(f\"\\nğŸ“¦ Test set:\")\n",
    "print(f\"   Objects: {objects[NUM_OBJECTS_TRAIN + NUM_OBJECTS_VALID:]}\")\n",
    "print(f\"   Shape: {X_test.shape}\")\n",
    "print(f\"   Truth samples: {np.sum(y_test == 0)}\")\n",
    "print(f\"   Lie samples: {np.sum(y_test == 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bxsaw6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4c: Train classifier\n",
    "print(\"ğŸ¤– Step 4c: Training binary classifier (Truth vs Lie)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Standardize features (important for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression classifier\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = clf.predict(X_train_scaled)\n",
    "y_valid_pred = clf.predict(X_valid_scaled)\n",
    "y_test_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, set_name):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ {set_name} Set Metrics:\")\n",
    "    print(f\"   Accuracy:  {acc:.4f} ({acc*100:.2f}%)\")\n",
    "    print(f\"   Precision: {prec:.4f}\")\n",
    "    print(f\"   Recall:    {rec:.4f}\")\n",
    "    print(f\"   F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "print(\"âœ… Classifier trained successfully!\")\n",
    "\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred, \"Training\")\n",
    "valid_metrics = calculate_metrics(y_valid, y_valid_pred, \"Validation\")\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred, \"Test\")\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\nğŸ” Feature Importance (Logistic Regression Coefficients):\")\n",
    "feature_names = ['diag_mean', 'diag_std', 'off_diag_mean', 'off_diag_std', 'max', 'min', 'upper_mean', 'lower_mean']\n",
    "for i, (name, coef) in enumerate(zip(feature_names, clf.coef_[0])):\n",
    "    print(f\"   {i+1}. {name:15s}: {coef:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ujb8dlvsk8m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4d: Evaluation and visualization\n",
    "print(\"ğŸ“Š Step 4d: Visualizing classifier performance\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# Create figure with confusion matrices and performance comparison\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Define grid for subplots\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Confusion matrices\n",
    "datasets = [\n",
    "    (\"Training\", y_train, y_train_pred, train_metrics),\n",
    "    (\"Validation\", y_valid, y_valid_pred, valid_metrics),\n",
    "    (\"Test\", y_test, y_test_pred, test_metrics)\n",
    "]\n",
    "\n",
    "for idx, (name, y_true, y_pred, metrics) in enumerate(datasets):\n",
    "    ax = fig.add_subplot(gs[0, idx])\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Truth', 'Lie'])\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    ax.set_title(f'{name} Set Confusion Matrix\\nAcc: {metrics[0]:.3f}, F1: {metrics[3]:.3f}')\n",
    "\n",
    "# Performance metrics comparison\n",
    "ax_metrics = fig.add_subplot(gs[1, :])\n",
    "metrics_data = {\n",
    "    'Training': train_metrics,\n",
    "    'Validation': valid_metrics,\n",
    "    'Test': test_metrics\n",
    "}\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(metric_names))\n",
    "width = 0.25\n",
    "\n",
    "for idx, (dataset_name, metrics) in enumerate(metrics_data.items()):\n",
    "    offset = (idx - 1) * width\n",
    "    ax_metrics.bar(x + offset, metrics, width, label=dataset_name, alpha=0.8)\n",
    "\n",
    "ax_metrics.set_ylabel('Score')\n",
    "ax_metrics.set_title('Classifier Performance Metrics Comparison')\n",
    "ax_metrics.set_xticks(x)\n",
    "ax_metrics.set_xticklabels(metric_names)\n",
    "ax_metrics.legend()\n",
    "ax_metrics.grid(axis='y', alpha=0.3)\n",
    "ax_metrics.set_ylim([0, 1.1])\n",
    "\n",
    "# Feature importance visualization\n",
    "ax_features = fig.add_subplot(gs[2, :])\n",
    "feature_names = ['diag_mean', 'diag_std', 'off_diag_mean', 'off_diag_std', 'max', 'min', 'upper_mean', 'lower_mean']\n",
    "coefficients = clf.coef_[0]\n",
    "colors = ['red' if c < 0 else 'green' for c in coefficients]\n",
    "\n",
    "bars = ax_features.barh(feature_names, coefficients, color=colors, alpha=0.7)\n",
    "ax_features.set_xlabel('Coefficient Value')\n",
    "ax_features.set_title('Feature Importance (Logistic Regression Coefficients)\\nPositive = More associated with Lie, Negative = More associated with Truth')\n",
    "ax_features.axvline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "ax_features.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add coefficient values to bars\n",
    "for i, (bar, coef) in enumerate(zip(bars, coefficients)):\n",
    "    ax_features.text(coef + (0.01 if coef > 0 else -0.01), bar.get_y() + bar.get_height()/2, \n",
    "                     f'{coef:+.3f}', \n",
    "                     ha='left' if coef > 0 else 'right', \n",
    "                     va='center', \n",
    "                     fontsize=9)\n",
    "\n",
    "plt.suptitle('Step 4: Binary Classifier Performance (Truth vs Lie Detection)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Save figure\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "save_path = os.path.join(export_dir, 'step4_classifier_performance.png')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"ğŸ“Š Figure saved to: {save_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… Step 4 complete! Classifier trained and evaluated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soomwl8x75q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4e: Per-object analysis on test set\n",
    "print(\"ğŸ” Step 4e: Analyzing classifier performance per test object\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get test object indices\n",
    "test_object_indices = object_indices[test_mask]\n",
    "test_objects = [objects[i] for i in range(NUM_OBJECTS_TRAIN + NUM_OBJECTS_VALID, len(objects))]\n",
    "\n",
    "print(f\"\\nğŸ“¦ Test objects: {test_objects}\")\n",
    "print()\n",
    "\n",
    "# Analyze each test object\n",
    "for obj_idx_in_test, obj_name in enumerate(test_objects):\n",
    "    actual_obj_idx = NUM_OBJECTS_TRAIN + NUM_OBJECTS_VALID + obj_idx_in_test\n",
    "    \n",
    "    # Get predictions for this object\n",
    "    obj_mask = test_object_indices == actual_obj_idx\n",
    "    y_obj_true = y_test[obj_mask]\n",
    "    y_obj_pred = y_test_pred[obj_mask]\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    obj_acc = accuracy_score(y_obj_true, y_obj_pred)\n",
    "    \n",
    "    # Count correct predictions\n",
    "    correct_truth = np.sum((y_obj_true == 0) & (y_obj_pred == 0))\n",
    "    correct_lie = np.sum((y_obj_true == 1) & (y_obj_pred == 1))\n",
    "    total_truth = np.sum(y_obj_true == 0)\n",
    "    total_lie = np.sum(y_obj_true == 1)\n",
    "    \n",
    "    print(f\"ğŸ“Œ {obj_name}:\")\n",
    "    print(f\"   Overall accuracy: {obj_acc:.3f} ({obj_acc*100:.1f}%)\")\n",
    "    print(f\"   Truth: {correct_truth}/{total_truth} correct ({correct_truth/total_truth*100:.1f}%)\")\n",
    "    print(f\"   Lie:   {correct_lie}/{total_lie} correct ({correct_lie/total_lie*100:.1f}%)\")\n",
    "    print()\n",
    "\n",
    "# Overall summary\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ¯ Overall Summary:\")\n",
    "print(f\"   Total objects: {len(objects)}\")\n",
    "print(f\"   Training objects: {NUM_OBJECTS_TRAIN} ({objects[:NUM_OBJECTS_TRAIN]})\")\n",
    "print(f\"   Validation objects: {NUM_OBJECTS_VALID} ({objects[NUM_OBJECTS_TRAIN:NUM_OBJECTS_TRAIN+NUM_OBJECTS_VALID]})\")\n",
    "print(f\"   Test objects: {len(objects) - NUM_OBJECTS_TRAIN - NUM_OBJECTS_VALID} ({test_objects})\")\n",
    "print(f\"   Responses per object: {NUM_RESPONSES_PER_OBJECT} (truth) + {NUM_RESPONSES_PER_OBJECT} (lie)\")\n",
    "print(f\"   Total samples: {len(X)}\")\n",
    "print(f\"\\n   Test Set Performance:\")\n",
    "print(f\"   - Accuracy:  {test_metrics[0]:.4f} ({test_metrics[0]*100:.2f}%)\")\n",
    "print(f\"   - Precision: {test_metrics[1]:.4f}\")\n",
    "print(f\"   - Recall:    {test_metrics[2]:.4f}\")\n",
    "print(f\"   - F1-Score:  {test_metrics[3]:.4f}\")\n",
    "print(\"\\nâœ… All tasks completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48241ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phimesasi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}